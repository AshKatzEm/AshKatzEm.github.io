<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
    <title>ESRB Rating</title>
    <link href="../styles.css" rel="stylesheet">
    </link>

</head>
<h1> Predicting ESRB Rating</h1>
<h2>Fall 2022 group project </h2>
<h3> <a href="https://github.com/AshKatzEm/SQLproject.git" >Github Repository</a>
</h3>

<body class="esrb">
  <p class="Introduction paragraph">
    I built a pipeline which scraped 58,000 rows of ESRB data, automatically moved it to a PostgreSQL database and then cleaned and transformed it into a CSV dataset. Afterwards, I did some upgraded modeling of it in python based on my previous experience modeling ESRB data.
  </p>
<!-- 
  •	Find a data warehouse. Possible snowflake or postgres
•	Build web scraper, possibly in a Docker
o	IT WILL CONNECT THE DATA WAREHOUSE
o	Using a credentials file 
o	INITIALIZE A TEMPORARY RAW DATA TABLE 
o	It will search a letter in ESRB site
o	Gather the 6 data points from each row
o	TRY TO  PASS THOSE POINTS AS A ROW INTO THE TEMPORARY TABLE, EITHER ONE AT A TIME OR ADD THEM TO A STRING WITH THE PROPER SQL FORMATTING AND PASS THAT WHOLE BIG THING LATER.
•	Working with at least 8 datasets, probably tons of overlap
•	Need to do a number of transformations.
o	Need to replace consoles in the middle of Interactive_Elements with ‘, ‘ [  \s\(.*\).*  and at the end with ‘.’
o	Load the date reference files:
	Try to standardize the console names in the Refrence tables
	Try one hot encoding the  
o	Use that to gather any release dates possible
o	Need to combine the rows where everything is the same but console into one row with a list of all the consoles.
•	Need to do pipeline transformations
o	Build into the scraper script a LOADER with sqlalchemy:
	A program which automatically connects to postgres
	Using a credentials file 
	Takes a scraped csv without downloading it (maybe generate a new one every 1000 rows)
	Creates a postgres table out of it
	Merges that table into a main table
	Performs all of the transformations
	Saves that main table 
o	The final Postgres table will be put on Kaggle
•	Additional Transformation DBT?
o	Create a #consoles feature
o	Create a #descriptors feature
o	Separate the consoles into one-hot columns
o	Separate the content descriptors into one-hot columns
o	Create a filler date column:
	See if the title has any year in it
	If not, add an average date
•	 make an average release date for a game on each console
•	Add up all of the avg date release dates and divide by the # consoles
	Or add a parametrized date:
•	Find the range of when games where coming out for each console
•	Find a year that is within the range of all the consoles
•	Maybe take the median of the in range years
•	Model the data in pandas 
•	Hook it up to the existing Streamlit visualization  -->
<!-- GridSearchCV and RandomizedSearchCV and DecisionTreeClassifier, RandomForestClassifier,HistGradientBoostingClassifier, XGBClassifier, GradientBoostingClassifier, LogisticRegression, GaussianNB -->

  <ol>
    <li>Decide which relational database to use.
      <p class="bullet-answer"> I decided to use PostGreSQL because I have past experience with it, it's free, and it's widely used and supported</p>
    </li>

    <li>Build a web scraper.
      <ul>
        <li>Connect to the website using Selenium.
          <p class="bullet-answer">I'm getting data from ESRB.com</p>
        </li>
        <li>Identify what we want to scrape
          <p class="bullet-answer">There are entries on the website for tens of thousands of games. Each game has six values associated with it, what I labled: game_title, esrb_rating, content_descriptors, interactive_elements, content_summary, platform,</p>
        </li>
        <li>Decide how to best harvest them.
          <p class="bullet-answer">We can directly add those values as a row in a Postgres table which I called RawData. I created a backup pandas DataFrame which periodically downloads as rawdata.csv</p>
        </li>
        <li>.</li>
      </ul>
    </li>
    <li>Connect the python web scraper to the relational database.
      <ul>
        <li>Identify which package[s] we will use to connect a python scrip to a PostGres database.
          <p class="bullet-answer">I like the Psychopg2 package for it's simplicity and similiarity to SQL relational database querying</p>
        </li>
        <li>Identify how to safely automate authentication and connection to the database.
          <p class="bullet-answer">I used a credentials.ini file, as well as a couple of python helper functions, configuration.py and connect.py, to ensure that the process consistently runs smoothly</p>
        </li>
      </ul>
    </li>
    <li>Build an ELT pipeline which moves the data into the warehouse, organizes it and transforms it.
      <ul>
      <li>Extracting the data.
        <p class="bullet-answer">Query the site and when a datapoint is found, add it as a row to a temporary SQL table called rawdata</p>
      </li>
      <li>Loading the data.
        <p> When the results of the query are completely parsed, add an id column and load the rawdata table into the PostGreSQL warehouse</p>
      </li>
      <li>Merge the data.
        <p class="bullet-answer">Join the rawdata table with previously collected data, eliminating any repeated datapoint which have been previously collected </p>
      </li>
      <li>Repeat these three steps until all queries have been parsed or favor is lost with the web gods.</li>
      <li>Transform the data:
        <p class="bullet-answer">This step automatically begins when the previous step is reached</p>
        <ol>
          <li>Remove any virtually identical rows</li>
          <li>Split the comma-seperated-list "platform" column into a more numerous one-platform-per-row column.</li>
          <li>Append ancillary data from external tables such as "release_date", "genres", and "sales". </li>
          <li>One-hot encode the comma-seperated-list "content_descriptors" column into numerous single-descriptor columns </li>
          <li>Output a csv file with all transformed data for modeling.
            <p class="bullet-answer"> Surprisingly challenging. I ultimately had to save it to the /tmp directory and then move it into a more convient spot.</p>
          </li>
        </ol>
      </li>
    </ul>
    </li>
    <li>Do more-complicated data wrangling in pandas which I want to try.
      <p class="bullet-answer"> There's now many rows missing data which couldn't be found in the ancillary tables</p>
      <ul>
        <li>Try to impute the 50% of the release_year which are Nan.
          <ol>
            <li>Create a dictionary which contains three pieces of information about each platform:
              <ul>
                <li>The year the platform was released</li>
                <li>The year the platform was discontinued</li>
                <li>The "generation" of the console</li>
              </ul>
            </li>
            <li>For each game which is missing a release_year, look for the game on another platform of the same generation but where release_year is present. If found, set the missing release_year to that found release_year</li>
            <li>For each game which all platforms, from the same generation, of the game have no release_year, for each set of games of the same platform generation, set the release_year to be the mean of the intersection years of the latest console release date (it couldn't have come out before that) and the earliest console discontinue date (it couldn't have come out after that).</li>

          </ol>
        </li>
        <li> Try to impute some of the 85% of sales data which is missing</li>
        <li>Make little tweaks to allow various types of models to train on the data</li>
      </ul>
    </li>
    <li>Decide what model to use with the dataset.</li>
      <ul>
        <li>I decided to manually evaluate various different models in a ipynb notebook. This way I could get a more intuative sense of the variance and data format preferences of each of the various models and take advantage of the unique information that some models provided along with their predictions. </li>
        <li> I explored seven different models:
          <ul>
            <li>RandomForestClassifier:</li>
            <p class="bullet-answer"> We ran with a RandomForestClassifier model last time we modeled ESRB, so the natural place to start would be to try the same model.
            what were the scores? There was a definite improvement in performance across all catagories. The overall accuracy increased to 88% by 2-3% and the variance was frustratingly improved. I had to put a decent amount of effort to move the model's perform by any registerable amount (11 sig fig), just to make sure the model was working correctly.</p> 
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/rfConf.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
              <img src="Images/RFPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
            </div>
            </div>
            


            <li>BernoulliNB</li>
            This is MultinomialNB but for binary features. I wanted to see how probablistic algorithms would work on the data, especially with much more data. In terms of performance, it performed a fair amount worse then RFC. The overall accuracy was 82% and struggled to positively identify Rated E10+. Further, and this happened with a couple other models, some strange is happening with the AO test samples. In this case, BNB only registers two "AO" test samples, when in reality there are 6. This doesn't change when I try rerunning the model. Since AO is a very small subset of the data, as of now 27 rows, it has virtually no effect on the model. Further, the reality is that, for practical reasons, AO is a death sentence for the commercial prospects of a game, so it seems to me that the ESRB and developers always work together to get any content to fit into a rated M, so it's really an academic catagory. Something similiar occured with the next model as well.
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/BNBConf.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
                <img src="Images/BNBPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
              </div>
            </div>


            <li>LogisticRegression</li>
            I tried this because I still wanted to see how probablistic algorithms would work on the data. Perhaps there's a certian threshold that can be found in the data. It performed about the same as NB, which isn't surprising. Here also, something strange happened with the AO test samples, it said there were no when in reality there were 6. This doesn't change when I try rerunning the model.
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/LRCConf.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
              <img src="Images/LRCPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
            </div>
            </div>

            <li>XGBClassifier</li>
            Firstly, Gradient Boosting is an algorithm that everyones talks about. It combines several weak learners into strong learners, in which each new model is trained to minimize some form of Gradient descent, each iteration hopefully stronger in a particular area than previously. Everyone raves about XG[radient]B[oosting] and it's frequenty said that it generally outperforms RF. I think in general, it is a top performer with continuous features but I wanted to try it out anyway. Suffice to say, vanilla XGB performed a bit worse then vanilla RF. With some simple hyper-parameter tuning, I got it performing nearly identical to vanilla RF.
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/XGBCConf.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
              <img src="Images/XGBCPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
            </div>
            </div>


            <li>HistGradientBoostingClassifier</li>
            XGB was a bit lack luster so I decided to try a more traditional gradient boosting algorithm. Traditionally, Gradient boosting was a slower training process, so I started with Histogram-Gradient_boosting, which speeds up the gradient descent by binning the data values. This is used for bigger datasets and maybe my dataset was pretty large. The performance was a fair amount worse than RF. It didn't take long to train however. Here also, something strange happened with the AO test samples. Unlike with the probability based models, Everytime I run this model, it shows up with a slightly different sample distribution. Further, one can tell something is not right because it says there are 88 AO samples, when the entire dataset has only 27. The other two GB models I tried did not exhibit this behavior, so I believe this algorithm is binning the samples by their features, taking the most inapporiate games and binning them into AO. I'm sure this could be corrected by adjusting the hyper parameters but I don't think this model is what we need so I'm not going to waste my time trying to do that.
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/HGBCConf.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
              <img src="Images/HGBCPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
            </div>
            </div>

            <li>GradientBoostingClassifier</li>
            Since HistogramGB didn't take long, I decided to try regular GB. It took a good deal longer with about the same performance as HistogramGB. I decided I was going to try to optimize the hyper parameters. After about the 13 minutes of run time, I got it performing nearly identical to vanilla RF.
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/GBC.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
              <img src="Images/GBCPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
            </div>
            </div>

            <li>DecisionTreeClassifier</li>
            If RF performed so well, maybe decision trees are well suited for this dataset. So I tried regulur Decision tree, which uses Gini Impurity to build a single good decision tree, compared to RF aggragating a "forest" of overfit trees. The performance was identical to RF. This was not the case when we modeled ESRB data a couple years ago. There might be some variance but it's always in range of the RF model
            <div class="row-models">
              <div class="column-models" >
                <img src="Images/DTCConf.png" class="Confusion" alt="Random Forest Confusion Matrix">
              </div>
              <div class="column-models" > 
              <img src="Images/DTCPerf.png" class="Perform" alt="Random Forest Confusion Matrix">
            </div>
            </div>
            


          </ul>
        </li>
        <li>I decided to roll with Random Forest it since quickly peforms the best with no variance. Can I improve the performance by optimizing the hyperparameters?</li>
        <ul>
          <li>RandomizedSearchCV</li>
          <li>GridSearchCV</li>
        </ul>
      </ul>
    <li>Does it improve the performance of the previous model</li>
    <li>Hook the new model up to streamlit</li>
  </ol>





    <iframe class="streamlit" src="https://sqlproject-2as5kapbpic5kicy56dj4f.streamlit.app/?embed=true"></iframe>

  



</body>

</html>

